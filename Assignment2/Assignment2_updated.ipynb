{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Machine Learning, Summer 2021 \n",
    "### Assignment 2: $\\ell_p$, logistic regression and quadratic neural nets\n",
    "\n",
    "__Given date:__ June 11\n",
    "\n",
    "__Due date:__ June 25\n",
    "\n",
    "__Total:__ 20pts + 5pts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1. (5pts) $\\ell_p$ norm minimization\n",
    "\n",
    "The use of the $\\ell_p$ norm can sometimes yield sparser solutions. In this exercise, we will consider the reconstruction of sparse signal from the minimization of the $\\ell_p$ norm for $p=0.5$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by considering a simple one dimensional signal $x$ with only a few non zero features (as shown below). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD4CAYAAADvsV2wAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAT9ElEQVR4nO3df2xd533f8fdXNO3cOO7oREpi0UqkAoa6ZFqqjnDsuRjcpJkcL5gEr1jsIWhaDHBaZFs7rDIsBOgPoIMNqBia1m0dIfOSbINjwFUVoXWrdnYAd0Mamw4T23HCxs0PWaRWM3FoOwlrSdS3f9zLlKLvpXh5z/35vF8AoXOfe3jO85xLfnj0nOc8JzITSdLo29LvCkiSesPAl6RCGPiSVAgDX5IKYeBLUiEu6XcF1rN169bcuXNnv6shSUPjiSee+HZmbmv23kAH/s6dO5menu53NSRpaETEt1q9Z5eOJBXCwJekQhj4klQIA1+SCmHgS1IhBnqUjgbfsZk5Dp+YZX5xie0TNQ7u282BvZP9rpakJgx8bdqxmTkOHX2KpbPLAMwtLnHo6FMAhr40gOzS0aYdPjH7w7BfsXR2mcMnZvtUI0nrMfC1afOLS22VS+ovA1+btn2i1la5pP4y8LVpB/ftpjY+dkFZbXyMg/t296lGktbjRVtt2sqF2TsefJIzy+eZdJSONNAMfHXkwN5J7n/sJAAPfOj6PtdG0nrs0pGkQhj4klQIA1+SCmHgS1IhDHxJKoSBL0mFMPAlqRAGviQVwsCXpEIY+JJUCANfkgpRSeBHxH0R8XxEPN3i/Rsj4sWI+GLj61er2K8kaeOqmjztE8A9wKfWWecvM/N9Fe1PktSmSs7wM/NR4IUqtiVJ6o5e9uFfHxFfiog/jYi3t1opIm6PiOmImF5YWOhh9SRptPVqPvwvAG/NzO9FxM3AMeCaZitm5hHgCMDU1FR2ozLHZuY4fGKW+cUltvvQDkmF6MkZfma+lJnfayw/BIxHxNZe7HutYzNzHDr6FHOLSyQwt7jEoaNPcWxmrh/VkaSe6UngR8SbIyIay9c29vudXux7rcMnZlk6u3xB2dLZZQ6fmO1HdSSpZyrp0omI+4Ebga0RcQr4NWAcIDPvBX4G+MWIOAcsAbdmZle6ay5mfnGprXJJGhWVBH5m3naR9++hPmyz77ZP1JhrEu7bJ2p9qI0k9U5xd9oe3Leb2vjYBWW18TEO7tvdpxpJUm/0apTOwFgZjXPHg09yZvk8k47SkVSI4gIf6qF//2MnAXjgQ9f3uTaS1BvFdelIUqmKPMOXesEb/DRoDHypC1Zu8Fu552PlBj/A0Fff2KUjdYE3+GkQGfhSF3iDnwaRXTpSF3iD3+DwWso/8Axf6oJRvMHv2MwcN9z9CLvu/BNuuPuRoZhw0MkSL2TgS11wYO8kd92yh0vH6r9ikxM17rplz9CeWQ5rcHot5UJ26UhdMko3+K0XnIP8R8xrKRfyDF/SRQ1rcLa6ZlLqtRQDX9JFDWtwjuK1lE4Y+JIualiDc9SupXTKPnxJFzXMs8yO0rWUThn4kjbE4Bx+Br6kltbetPSaS7aw9YrL+l0tbZKBL6mpZhPAbYk+V0od8aKtpKaajb0/n/Dcdwd7KKZaM/AlNdVqjP2Z5fM9romqYuBLaqrVGPuVIY4aPn5ykppqNvZ+S8COKwf7Ziu15kVbSU01G3vvKJ3h5hm+pJYO7J1k71smeOeu1/P/7nyXYT/kDHxJKoSBL0mFMPAlqRAGviQVopLAj4j7IuL5iHi6xfsREb8TEc9GxJMR8RNV7FcaBMP4rFeVqaoz/E8AN63z/nuBaxpftwN/UNF+pb4a1me9qkyVBH5mPgq8sM4q+4FPZd1fARMRcVUV+5b6yYdka5j0qg9/Enhu1etTjbJXiYjbI2I6IqYXFhZ6Ujlps4b1Wa8qU68Cv9mkqtlsxcw8kplTmTm1bdu2LldL6sywPutVZepV4J8Cdqx6fTUw36N9S10zrM96VZl6FfjHgZ9tjNa5DngxM0/3aN9S1/iQbA2TSiZPi4j7gRuBrRFxCvg1YBwgM+8FHgJuBp4FfgD8fBX7lQaBz3rVsKgk8DPztou8n8CHq9iXJGlzvNNWkgph4EtSIQx8SSqEgS9JhTDwJakQBr4kFcLAl6RCGPiSVAgDX5IKUcmdttIoOjYzx+ETs8wvLrF9osbBfbudI0dDzcCXmlh5ktXKw01WnmQFGPoaWnbpSE34JCuNIgNfasInWWkUGfhSEz7JSqPIwJea8ElWGkVetJWaWLkwe8eDT3Jm+TyTjtLRCDDwpRZ8kpVGjV06klQIA1+SCmHgS1IhDHxJKoSBL0mFMPAlqRAGviQVwsCXpEIY+JJUCANfkgph4EtSIQx8SSpEJYEfETdFxGxEPBsRdzZ5/8aIeDEivtj4+tUq9itJ2riOZ8uMiDHg94D3AKeAxyPieGY+s2bVv8zM93W6P0nS5lRxhn8t8Gxmfj0zzwCfBvZXsF1JUoWqCPxJ4LlVr081yta6PiK+FBF/GhFvb7WxiLg9IqYjYnphYaGC6kmSoJrAjyZlueb1F4C3ZuY7gN8FjrXaWGYeycypzJzatm1bBdWTJEE1gX8K2LHq9dXA/OoVMvOlzPxeY/khYDwitlawb0nSBlUR+I8D10TEroi4FLgVOL56hYh4c0REY/naxn6/U8G+JUkb1PEoncw8FxH/ATgBjAH3ZeaXI+IXGu/fC/wM8IsRcQ5YAm7NzLXdPpKkLqrkIeaNbpqH1pTdu2r5HuCeKvYlSdoc77SVpEIY+JJUCANfkgph4EtSIQx8SSpEJaN0JKkfjs3McfjELPOLS2yfqHFw324O7G02s4vAwJc0pI7NzHHo6FMsnV0GYG5xiUNHnwIw9FuwS0fSUDp8YvaHYb9i6ewyh0/M9qlGg8/AlzSU5heX2iqXgS9pSG2fqLVVLgNf0pA6uG83tfGxC8pq42Mc3Le7TzUafF60lTSUVi7M3vHgk5xZPs+ko3QuysCXNLQO7J3k/sdOAvDAh67vc20Gn106klQIA1+SCmHgS1IhDHxJKoSBL0mFMPAlqRAGviQVwnH4Uo85pa/6xcCXesgpfdVPdulIPeSUvuonA1/qIaf0VT8Z+FIPOaWv+snAl3rIKX3VT160lXrIKX3VTwa+RtogDoF0St/2DeLnOIwMfI0sh0COBj/H6lQS+BFxE/BRYAz4eGbeveb9aLx/M/AD4Ocy8wtV7Hut1WcC/6g2TgQs/uDsBcvbJ2q85pItbL3isg2tv5Hl7RM1furHtvHZry50vK1uLHerfoN8LL/7g7Ov+vlYOrvMLz/wRX79+Jc33ba19VtvvXbK1+575Sy2k+Pa6bFcXdd2f882uv3VZ+vNtrve53j4xOyGjmUVbWj2uVfxGa13PKoWmdnZBiLGgL8G3gOcAh4HbsvMZ1atczPwH6kH/juBj2bmOy+27ampqZyent5wXdaeCaxnS8DWyy/l5VeWN7S+WhvlY7nRtrVar93y1WrjY/ybfzbJHz4x19fjuiVg1xsu5+FfuZH3f+xzfPvlV5h/8e8qq1NtfIy7btkDsOHf37X1u9ixXN0GaC8rLlb3qj+jleOx2dCPiCcyc6rZe1Wc4V8LPJuZX2/s7NPAfuCZVevsBz6V9b8ufxURExFxVWaermD/P7RyU8uHnvwMP/riXJWblvoiPhvs6fCkrJJ6RPCtL93Hz51+iZf/7hydnii+yqP1kUu/fq57f9hW2gDAycXK9tWNz+jk02+FP/r9SrcJ1QzLnASeW/X6VKOs3XUAiIjbI2I6IqYXFhbaqog3r2jUVB6sm7RSj7dd9SNdqdMr55Z5pYthD//QhmdOv1TpvrpxPL7/yrnKtwnVnOFHk7K1R2Aj69QLM48AR6DepdNORbZP1JhbXOJj/3T/htYfi2B5QH6hht0oH8uNtq3Veu2WD6LJiRrvv/NdAPy7ux9hruKTq0vH6ueeZ5bPb+r7N3IsV9pwx8c+x8zJxU3vqxcuHdvCoS5st4oz/FPAjlWvrwbmN7FOx5rd1NJKbXyM2965Y8Prq7VBP5a18TE+cN1bNlW/jbat1Xrtlq+2JeCNr7u078d17Y1h7fyebXT7O66ssePK2rrbbfU5buRYrm3Dxfa1Ud34jLZEvX7dUEXgPw5cExG7IuJS4Fbg+Jp1jgM/G3XXAS9W3X8P9SFad92yh8mJGgFM1Ma58rXjr1qenKhx1y17+M0Deza0/kaWJydqfOC6t1SyrW4sd6t+g34sO6lfq+9dW7/11munfO2+d73hcnZte11Hx7XTY7lS19UXEDf6e9bO9rdecRlbr7hs3ePRybFcacOxmTlmTi7yN9/+PpddsqWjY1nVZ9Tsc187oqgqHY/SgR+Owvlt6sMy78vM/xoRvwCQmfc2hmXeA9xEfVjmz2fmRYfftDtKRxol7//Y54Aybs7qRVubjczpdERMN+rd6Ta7PUqHzHwIeGhN2b2rlhP4cBX7kqTNWG9q6lJu4HLyNElFcGpqA19SIZya2sCXVAinpnbyNEmFWOmnL3nWTQNfUjEO7J0sKuDXsktHkgph4EtSIQx8SSqEgS9JhTDwJakQBr4kDYiVyd0+/40XuOHuRzg2U+2DnAx8SRoAK5O7rczTv/Kw9ipD38CXpE2o+mx8vcndqmLgS1KbunE23ovJ3Qx8SWpTN87GezG5m4EvSW3qxtl4LyZ3M/AlqU3dOBtf++jIZo+X7JSTp0lSmw7u2930cYmdno13e3I3A1+S2jSsUy0b+JK0CcM41bJ9+JJUCANfkgph4EtSIQx8SSqEgS9JhTDwJakQBr4kFcLAl6RCGPiSVIiO7rSNiNcDDwA7gW8C/zYzv9tkvW8CLwPLwLnMnOpkv5Kk9nV6hn8n8HBmXgM83Hjdyk9l5o8b9pLUH50G/n7gk43lTwIHOtyeJKlLOg38N2XmaYDGv29ssV4Cfx4RT0TE7ettMCJuj4jpiJheWFjosHqSpBUX7cOPiP8DvLnJWx9pYz83ZOZ8RLwR+IuI+GpmPtpsxcw8AhwBmJqayjb2IUlax0UDPzN/utV7EfG3EXFVZp6OiKuA51tsY77x7/MR8UfAtUDTwJckdUenXTrHgQ82lj8IfGbtChFxeURcsbIM/Evg6Q73K0lqU6eBfzfwnoj4GvCexmsiYntEPNRY503A/42ILwGPAX+SmX/W4X4lSW3qaBx+Zn4HeHeT8nng5sby14F3dLIfSVLnvNNWkgph4EtSIQx8SSqEgS9JhTDwJakQBr4kFcLAl6RCGPiSVAgDX5IKYeBLUiEMfEkqhIEvSYUw8CWpEAa+JBXCwJcG0LGZOWZOLvL5b7zADXc/wrGZuX5XSSPAwJcGzLGZOQ4dfYozy+cBmFtc4tDRpwx9dczAlwbM4ROzLJ1dvqBs6ewyh0/M9qlGGhUGvjRg5heX2iqXNsrAlwbM9olaW+XDzusVvWPgSwPm4L7d1MbHLiirjY9xcN/uPtWoe7xe0VsGvjRgDuyd5K5b9jA5USOAyYkad92yhwN7J/tdtcp5vaK3Lul3BSS92oG9kyMZ8Gt5vaK3PMOX1DelXa/oNwNfUt+UdL1iENilI6lvVrqtDp+YZX5xie0TNQ7u211Ed1Y/GPiS+qqU6xWDwC4dSSqEgS9JhTDwJakQBr4kFcLAl6RCRGb2uw4tRcQC8K1NfvtW4NsVVmcYlNhmKLPdJbYZymx3u21+a2Zua/bGQAd+JyJiOjOn+l2PXiqxzVBmu0tsM5TZ7irbbJeOJBXCwJekQoxy4B/pdwX6oMQ2Q5ntLrHNUGa7K2vzyPbhS5IuNMpn+JKkVQx8SSrEyAV+RNwUEbMR8WxE3Nnv+nRLROyIiM9GxFci4ssR8UuN8tdHxF9ExNca/17Z77pWLSLGImImIv648bqENk9ExIMR8dXGZ379qLc7Iv5z42f76Yi4PyJeM4ptjoj7IuL5iHh6VVnLdkbEoUa+zUbEvnb2NVKBHxFjwO8B7wXeBtwWEW/rb6265hzwXzLzHwPXAR9utPVO4OHMvAZ4uPF61PwS8JVVr0to80eBP8vMHwPeQb39I9vuiJgE/hMwlZn/BBgDbmU02/wJ4KY1ZU3b2fgdvxV4e+N7fr+RexsyUoEPXAs8m5lfz8wzwKeB/X2uU1dk5unM/EJj+WXqATBJvb2fbKz2SeBAXyrYJRFxNfCvgI+vKh71Nv8I8C+A/w6QmWcyc5ERbzf153XUIuIS4LXAPCPY5sx8FHhhTXGrdu4HPp2Zr2TmN4Bnqefehoxa4E8Cz616fapRNtIiYiewF/g88KbMPA31PwrAG/tYtW74beAO4PyqslFv848CC8D/aHRlfTwiLmeE252Zc8BvASeB08CLmfnnjHCb12jVzo4ybtQCP5qUjfS404h4HfCHwC9n5kv9rk83RcT7gOcz84l+16XHLgF+AviDzNwLfJ/R6MpoqdFnvR/YBWwHLo+ID/S3VgOho4wbtcA/BexY9fpq6v8NHEkRMU497P93Zh5tFP9tRFzVeP8q4Pl+1a8LbgD+dUR8k3p33bsi4n8x2m2G+s/1qcz8fOP1g9T/AIxyu38a+EZmLmTmWeAo8M8Z7Tav1qqdHWXcqAX+48A1EbErIi6lfnHjeJ/r1BUREdT7dL+Smf9t1VvHgQ82lj8IfKbXdeuWzDyUmVdn5k7qn+0jmfkBRrjNAJn5/4HnImJ3o+jdwDOMdrtPAtdFxGsbP+vvpn6dapTbvFqrdh4Hbo2IyyJiF3AN8NiGt5qZI/UF3Az8NfA3wEf6XZ8utvMnqf9X7kngi42vm4E3UL+q/7XGv6/vd1271P4bgT9uLI98m4EfB6Ybn/cx4MpRbzfwG8BXgaeB/wlcNoptBu6nfp3iLPUz+H+/XjuBjzTybRZ4bzv7cmoFSSrEqHXpSJJaMPAlqRAGviQVwsCXpEIY+JJUCANfkgph4EtSIf4e0ze9aJYMO0UAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "x0 = np.zeros((100,1))\n",
    "\n",
    "ind = np.random.choice(np.arange(0,len(x0)), 20)\n",
    "\n",
    "x0[ind] = np.random.normal(0,1,np.shape(ind)).reshape(-1,1)\n",
    "\n",
    "plt.stem(x0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will consider the recovery of the vector above given measurements of the form $\\mathbf{y} = A\\mathbf{x}$. In this case, we will take the matrix $A$ to be given by a random subset of the rows of the DFT matrix given below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(x0)\n",
    "import numpy as np\n",
    "pi=np.pi\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "def DFT_matrix(N):\n",
    "    i, j = np.meshgrid(np.arange(N), np.arange(N))\n",
    "    omega = np.exp( - 2 * pi * 1J / N )\n",
    "    W = np.power( omega, i * j ) / np.sqrt(N)\n",
    "    return W\n",
    "\n",
    "measurements = np.matmul(DFT_matrix(len(x0)),(x0.reshape(-1,1)))\n",
    "\n",
    "idx = np.random.choice(len(x), size = 30, replace = False)\n",
    "A = DFT_matrix(len(x0))[idx,:]\n",
    "\n",
    "w, v = np.linalg.eig(np.matmul(A, np.conjugate(A).T))\n",
    "b = measurements[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We thus want to solve the problem\n",
    "\n",
    "\\begin{align}\n",
    "\\min &\\quad \\left\\|\\mathbf{x}\\right\\|_{p}\\\\\n",
    "\\text{s.t.} & \\quad A \\mathbf{x} = \\mathbf{b} \n",
    "\\end{align}\n",
    "\n",
    "On approach is to rely on a projection of the gradient onto the constraint $A\\mathbf{x} = b$. Projection onto the constraint can be done through the step \n",
    "\n",
    "\\begin{align}\n",
    "\\min_{\\mathbf{x}} &\\quad \\left\\|\\mathbf{x} - \\mathbf{x}^*\\right\\|_2\\\\\n",
    "\\text{subject to} & \\quad A\\mathbf{x} = \\mathbf{b}\n",
    "\\end{align}\n",
    "\n",
    "where $\\mathbf{x}^*$ is the current point. The solution to the above problem can be computed in closed form as \n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{x} = \\mathbf{x}^* + A^*(AA^*)^{-1}(b - Ax^*)\n",
    "\\end{align}\n",
    "\n",
    "where $A^*$ is the adjoint (conjugate transpose) of the matrix $A$. The iterations then take the following form\n",
    "\\begin{align}\n",
    "\\mathbf{x} &\\leftarrow \\mathbf{x} - \\eta*d_k\\\\\n",
    "\\mathbf{x} &\\leftarrow \\mathcal{P}_{A\\mathbf{x} = \\mathbf{b}}(\\mathbf{x})\\\\\n",
    "\\end{align}\n",
    "\n",
    "Here $\\mathcal{P}_{A\\mathbf{x} = \\mathbf{b}}(\\mathbf{x})$ corresponds to $\\mathbf{x}^* + A^*(AA^*)^{-1}(b - Ax^*)$ and if we minimize $\\frac{1}{p}\\|\\mathbf{x}\\|_p^p$, the gradient can be written as \n",
    "\\begin{align}\n",
    "(d_k)_\\ell = |\\mathbf{x}_\\ell|^{p-2} \\mathbf{x}_\\ell\n",
    "\\end{align}\n",
    "\n",
    "to avoid division by $0$ we will replace $|\\mathbf{x}_\\ell|$ with $\\sqrt{|\\mathbf{x}_\\ell|^2 + \\varepsilon^2}$ for some small $\\varepsilon$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 1.1.__ Apply a few steps of the projected gradient method (1000) and compare the results for $p = 1, 2$ and $p=0.5$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2. (10pts) Logistic regression for digit recognition\n",
    "\n",
    "When considering logistic regression, one approach is to apply gradient updates on the log likelihood. In this exercise, we will use logistic regression to discriminate between two digits. Use the lines below to load the MNIST data from scikit learn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "X, y = fetch_openml('mnist_784', version=1, return_X_y=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Images from the original MNIST dataset are of size 28 by 28. Those images are stored as the rows of the X matrix above as 784 feature vectors. In this exercise, you will learn a classifier to discriminate between images of the 0 digit and of the digit 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 2.1 (1pts).__ Start by gathering all the images of 0 and 1 by using the target vector $y$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 2.2. (1pts)__ Once you have the images, fill out the expression of the function \n",
    "sigmoid below wich should return the value of the function and its derivative for a _vector_ $\\mathbf{x}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 2.3. (4pts)__ Using the function sigmoid above, write a code that returns the expression of the log likelihood for an image $\\mathbf{x}_i$ with associated target $y_i = 0, 1$. Then compute the gradient of this loss.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 2.4. (4pts)__ Run SGD on the log likelihood and plot the evolution of the log loss throughout the iterations. Take a sufficiently small learning rate (0.005 - 0.0025) and consider multiple epochs (~15) with batch size (10 - 100) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3 (5pts) Intriguing properties of quadratic neural networks\n",
    "\n",
    "We consider a simple shallow network with quadratic activation functions. We will repeat the following experiments:\n",
    "\n",
    "1- Generate a random dataset in dimension $d$ for $d = 2, 3, 5$ and $10$ by taking $\\mathbf{x}_i \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})$ (multivariate Gaussian with mean 0 and covariance given by the identity), and $t_i\\sim \\mathcal{N}(0,1)$\n",
    "\n",
    "2- We consider a shallow neural network with quadratic activations as given below\n",
    "\n",
    "\\begin{align}\n",
    "h_{\\mathbf{v}, \\mathbf{W}}(\\mathbf{x}) = \\sum_{\\ell=1}^k \\mathbf{v}_\\ell \\phi(\\langle \\mathbf{w}_\\ell, \\mathbf{x} \\rangle)\n",
    "\\end{align}\n",
    "\n",
    "Such a network can be written compactly by stacking the vectors $\\mathbf{w}_\\ell$ and scalars $v_\\ell$ in the matrix $\\mathbf{W}$ and vector $\\mathbf{v}$ as \n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{W} = \\left[\\begin{array}{c}\n",
    "\\mathbf{w}_1^T\\\\\n",
    "\\mathbf{w}_2^T \\\\\n",
    "\\vdots\\\\\n",
    "\\mathbf{w}_k^T\n",
    "\\end{array}\\right]\\quad \\mathbf{v} = \\left[\\begin{array}{c}\n",
    "v_1 \\\\\n",
    "v_2 \\\\\n",
    "\\vdots \\\\\n",
    "v_k\n",
    "\\end{array}\\right]\n",
    "\\end{align} \n",
    "From this, we can write the model as \n",
    "\\begin{align}\n",
    "h_{\\mathbf{v}, \\mathbf{W}}(\\mathbf{x}) = \\mathbf{v}^T \\phi(\\mathbf{W}\\mathbf{x})\n",
    "\\end{align}\n",
    "\n",
    "We consider the minimization of the loss (for any $\\mathbf{v}$):\n",
    "\n",
    "\\begin{align}\n",
    "\\ell(\\mathbf{W}) \\frac{1}{2n} \\sum_{i=1}^n \\left(y_i - \\mathbf{v}^T\\phi(\\mathbf{W}\\mathbf{x}_i)\\right)^2\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "3- Take $d\\leq n\\leq c d^2$ and start gradient descent on the loss given above for $\\mathbf{v}=\\mathbf{1}$ and $\\mathbf{W}$ generated as $\\mathbf{W}_{ij}\\sim \\mathcal{N}(0,1)/\\sqrt{d}$. wait for the iterations to converge. Repeat your experiments for several choices of $\\mathbf{W}$, $d$ and $k$. Start by taking $k$ smaller than $k$, then gradually increase $k$ so that it ultimately becomes larger than $d$. Plot the evolution of the loss for each experiment. Compare the evolution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating the dataset\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
